## 목표


프로젝트의 최종 목표인 책의 추천 및 검색 기능을 도입하기 위해 많은 책 데이터가 필요. 

**국립중앙도서관 API**를 통해 수만에서 수십만건의 책 정보를 불러와 DB에 저장하여 그 DB를 기반으로 이후 개발 진행.

## 문제


적당히 많은 책의 데이터를 넣어 프로젝트를 가동해보는 것이 실제와 비슷한 경험을 할 수 있다고 생각하여 약 10~20만건의 책 데이터를 불러와 DB에 저장하기로 했을 때 생길 수 있는 문제점.

1. API에서 한번에 최대 2000건의 책 DATA를 불러올 수 있어 20만건을 저장하기 위해선 다수의 호출이 필요
2. 10~20만건의 책을 저장하려 한다면 20번의 DB 저장 프로세스는 성능 저하 및 부하를 초래할 수 있음

## 해결 방안

1. 설계

API 통해 불러온 책 데이터를 하나하나 저장하는 것보다 묶음 처리를 하여 한번에 DB에 저장하는 방식으로 호출 수를 대폭 줄여 성능 향상을 의도하도록 함. 이 과정에서 묶음 처리가 너무 클 때 발생 할 수 있는 캐싱 부하 문제도 고려하여 한번에 500건 정도를 저장하기로 결정. API를 통한 호출도 500건씩 여러번 요청을 한번에 보내도 부하의 문제가 있을 수 있어 요청 당 1초의 간격을 두고 요청하도록 함.

2. 해결 구현

현재 Repository가 JPA로 만들어져있어 JPA에서 사용 할 수 있는 saveAll() 기능은 save()를 반복하는 작업이라 묶음 처리가 될 수 없음. BatchSize를 변경하고 Jpa의 BatchUpdate를 사용은 Book Entity의 Id 
`@GeneratedValue(strategy = GenerationType.IDENTITY)`

와 충돌하여 작동하지 못함.

DB에 대량의 데이터가 저장되는 것은 자주 이용되는 함수가 아니기에 JdbcTemplate의 batchUpdate를 이용. 삽입용 쿼리문을 직접 작성해야 하는 번거로움이 존재하고, 내용이 변경되면 유지 보수가 JPA를 이용했을 때 보다는 어려워지지만, 해당 건은 번거로움보다 서버 부하와 메모리 부하를 절감, 또한 소요되는 시간을 줄이는 것이 더 중요하다고 생각하여 Bulk Insert를 이용

## 결론


JPA는 다량의 데이터를 삽입할 때 성능상 불리한 경우가 많은거 같습니다.

영속성 컨텍스트 유지비용, saveAll 메서드의 한계, ID 생성 전략에 따라 Bulk Insert의 불가, 느린 속도.

이것을 Jdbc Template의 batchUpdate 메서드를 이용함으로써, JPA를 이용할 때보다 빠르고 적은 요청 수로 다량의 데이터를 입력할 수 있었습니다.
